{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-16T07:48:37.879298Z",
     "iopub.status.busy": "2020-12-16T07:48:37.878507Z",
     "iopub.status.idle": "2020-12-16T07:48:40.056374Z",
     "shell.execute_reply": "2020-12-16T07:48:40.055750Z"
    },
    "papermill": {
     "duration": 2.215605,
     "end_time": "2020-12-16T07:48:40.056515",
     "exception": false,
     "start_time": "2020-12-16T07:48:37.840910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014453,
     "end_time": "2020-12-16T07:48:40.086657",
     "exception": false,
     "start_time": "2020-12-16T07:48:40.072204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Load the dataset.\n",
    "Here, we have dataset with 50k reviews(equally distributed, 25k positive and 25k negative)\n",
    "you can see the top 5 reviews in the set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-16T07:48:40.130702Z",
     "iopub.status.busy": "2020-12-16T07:48:40.124894Z",
     "iopub.status.idle": "2020-12-16T07:48:41.637233Z",
     "shell.execute_reply": "2020-12-16T07:48:41.637750Z"
    },
    "papermill": {
     "duration": 1.536203,
     "end_time": "2020-12-16T07:48:41.637914",
     "exception": false,
     "start_time": "2020-12-16T07:48:40.101711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T07:48:41.686184Z",
     "iopub.status.busy": "2020-12-16T07:48:41.685285Z",
     "iopub.status.idle": "2020-12-16T07:48:41.704713Z",
     "shell.execute_reply": "2020-12-16T07:48:41.705251Z"
    },
    "papermill": {
     "duration": 0.051386,
     "end_time": "2020-12-16T07:48:41.705403",
     "exception": false,
     "start_time": "2020-12-16T07:48:41.654017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "positive = reviews[reviews['sentiment']=='positive']\n",
    "negative = reviews[reviews['sentiment']=='negative']\n",
    "print(positive.shape)\n",
    "print(negative.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017606,
     "end_time": "2020-12-16T07:48:41.739301",
     "exception": false,
     "start_time": "2020-12-16T07:48:41.721695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Check if there is any row which has NaN and if yes, remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T07:48:41.792829Z",
     "iopub.status.busy": "2020-12-16T07:48:41.786035Z",
     "iopub.status.idle": "2020-12-16T07:48:41.800954Z",
     "shell.execute_reply": "2020-12-16T07:48:41.800118Z"
    },
    "papermill": {
     "duration": 0.045084,
     "end_time": "2020-12-16T07:48:41.801083",
     "exception": false,
     "start_time": "2020-12-16T07:48:41.755999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = reviews[reviews['review'].notnull() & reviews['sentiment'].notnull()]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T07:48:41.844042Z",
     "iopub.status.busy": "2020-12-16T07:48:41.843170Z",
     "iopub.status.idle": "2020-12-16T07:48:59.546844Z",
     "shell.execute_reply": "2020-12-16T07:48:59.546112Z"
    },
    "papermill": {
     "duration": 17.729511,
     "end_time": "2020-12-16T07:48:59.546974",
     "exception": false,
     "start_time": "2020-12-16T07:48:41.817463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_features = []\n",
    "\n",
    "for sentence in range(0, len(features)):\n",
    "    # Remove all the special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
    "\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "\n",
    "    processed_features.append(processed_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016285,
     "end_time": "2020-12-16T07:48:59.581388",
     "exception": false,
     "start_time": "2020-12-16T07:48:59.565103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the script above, we start by removing all the special characters from the reviews. The regular expression re.sub(r'\\W', ' ', str(features[sentence])) does that.\n",
    "\n",
    "Next, we remove all the single characters left as a result of removing the special character using the re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature) regular expression. For instance, if we remove special character ' from Jack's and replace it with space, we are left with Jack s. Here s has no meaning, so we remove it by replacing all single characters with a space.\n",
    "\n",
    "However, if we replace all single characters with space, multiple spaces are created. Therefore, we replace all the multiple spaces with single spaces using re.sub(r'\\s+', ' ', processed_feature, flags=re.I) regex. Furthermore, if your text string is in bytes format a character b is appended with the string. The above script removes that using the regex re.sub(r'^b\\s+', '', processed_feature).\n",
    "\n",
    "Finally, the text is converted into lowercase using the lower() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015975,
     "end_time": "2020-12-16T07:48:59.613448",
     "exception": false,
     "start_time": "2020-12-16T07:48:59.597473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Representing Text in Numeric Form\n",
    "\n",
    "Statistical algorithms use mathematics to train machine learning models. However, mathematics only work with numbers. To make statistical algorithms work with text, we first have to convert text to numbers. To do so, three main approaches exist i.e. Bag of Words, TF-IDF and Word2Vec. In this section, we will discuss the bag of words and TF-IDF scheme.\n",
    "\n",
    "**Bag of Words**\n",
    "\n",
    "Bag of words scheme is the simplest way of converting text to numbers.\n",
    "\n",
    "For instance, you have three documents:\n",
    "\n",
    "    Doc1 = \"I like to play football\"\n",
    "    Doc2 = \"It is a good game\"\n",
    "    Doc3 = \"I prefer football over rugby\"\n",
    "\n",
    "In the bag of words approach the first step is to create a vocabulary of all the unique words. For the above three documents, our vocabulary will be:\n",
    "\n",
    "Vocab = [I, like, to, play, football, it, is, a, good, game, prefer, over, rugby]\n",
    "\n",
    "The next step is to convert each document into a feature vector using the vocabulary. The length of each feature vector is equal to the length of the vocabulary. The frequency of the word in the document will replace the actual word in the vocabulary. If a word in the vocabulary is not found in the corresponding document, the document feature vector will have zero in that place. For instance, for Doc1, the feature vector will look like this:\n",
    "\n",
    "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "**TF-IDF**\n",
    "\n",
    "In the bag of words approach, each word has the same weight. The idea behind the TF-IDF approach is that the words that occur less in all the documents and more in individual document contribute more towards classification.\n",
    "\n",
    "TF-IDF is a combination of two terms. Term frequency and Inverse Document frequency. They can be calculated as:\n",
    "\n",
    "TF  = (Frequency of a word in the document)/(Total words in the document)\n",
    "\n",
    "IDF = Log((Total number of docs)/(Number of docs containing the word))\n",
    "\n",
    "TF-IDF using the Scikit-Learn Library\n",
    "\n",
    "Luckily for us, Python's Scikit-Learn library contains the TfidfVectorizer class that can be used to convert text features into TF-IDF feature vectors. The following script performs this:\n",
    "\n",
    "In the code above, we define that the max_features should be 2500, which means that it only uses the 2500 most frequently occurring words to create a bag of words feature vector. Words that occur less frequently are not very useful for classification.\n",
    "\n",
    "Similarly, max_df specifies that only use those words that occur in a maximum of 80% of the documents. Words that occur in all documents are too common and are not very useful for classification. Similarly, min-df is set to 7 which shows that include words that occur in at least 7 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T07:48:59.652213Z",
     "iopub.status.busy": "2020-12-16T07:48:59.651515Z",
     "iopub.status.idle": "2020-12-16T07:49:14.684302Z",
     "shell.execute_reply": "2020-12-16T07:49:14.682799Z"
    },
    "papermill": {
     "duration": 15.054957,
     "end_time": "2020-12-16T07:49:14.684440",
     "exception": false,
     "start_time": "2020-12-16T07:48:59.629483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer (max_features=3500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "processed_features = vectorizer.fit_transform(processed_features).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01631,
     "end_time": "2020-12-16T07:49:14.717321",
     "exception": false,
     "start_time": "2020-12-16T07:49:14.701011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Divide Dataset into Train And Test Set\n",
    "\n",
    "In the previous section, we converted the data into the numeric form. As the last step before we train our algorithms, we need to divide our data into training and testing sets. The training set will be used to train the algorithm while the test set will be used to evaluate the performance of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T07:49:14.766169Z",
     "iopub.status.busy": "2020-12-16T07:49:14.765460Z",
     "iopub.status.idle": "2020-12-16T07:49:15.438534Z",
     "shell.execute_reply": "2020-12-16T07:49:15.439132Z"
    },
    "papermill": {
     "duration": 0.704772,
     "end_time": "2020-12-16T07:49:15.439304",
     "exception": false,
     "start_time": "2020-12-16T07:49:14.734532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = reviews['sentiment']\n",
    "labels = labels=='positive'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016374,
     "end_time": "2020-12-16T07:49:15.472293",
     "exception": false,
     "start_time": "2020-12-16T07:49:15.455919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the Model\n",
    "Once data is split into training and test set, machine learning algorithms can be used to learn from the training data. You can use any machine learning algorithm. However, we will use the Random Forest algorithm, owing to its ability to act upon non-normalized data.\n",
    "\n",
    "The sklearn.ensemble module contains the RandomForestClassifier class that can be used to train the machine learning model using the random forest algorithm. To do so, we need to call the fit method on the RandomForestClassifier class and pass it our training features and labels, as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T07:49:15.511781Z",
     "iopub.status.busy": "2020-12-16T07:49:15.511009Z",
     "iopub.status.idle": "2020-12-16T07:52:35.729299Z",
     "shell.execute_reply": "2020-12-16T07:52:35.729844Z"
    },
    "papermill": {
     "duration": 200.240837,
     "end_time": "2020-12-16T07:52:35.730024",
     "exception": false,
     "start_time": "2020-12-16T07:49:15.489187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016942,
     "end_time": "2020-12-16T07:52:35.764084",
     "exception": false,
     "start_time": "2020-12-16T07:52:35.747142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Making Predictions and Evaluating the Model\n",
    "\n",
    "Once the model has been trained, the last step is to make predictions on the model. To do so, we need to call the predict method on the object of the RandomForestClassifier class that we used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T07:52:35.804041Z",
     "iopub.status.busy": "2020-12-16T07:52:35.803325Z",
     "iopub.status.idle": "2020-12-16T07:52:37.547875Z",
     "shell.execute_reply": "2020-12-16T07:52:37.547120Z"
    },
    "papermill": {
     "duration": 1.766613,
     "end_time": "2020-12-16T07:52:37.547998",
     "exception": false,
     "start_time": "2020-12-16T07:52:35.781385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017136,
     "end_time": "2020-12-16T07:52:37.583543",
     "exception": false,
     "start_time": "2020-12-16T07:52:37.566407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, to evaluate the performance of the machine learning models, we can use classification metrics such as a confusion metrix, F1 measure, accuracy, etc.\n",
    "\n",
    "To find the values for these metrics, we can use classification_report, confusion_matrix, and accuracy_score utilities from the sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T07:52:37.625738Z",
     "iopub.status.busy": "2020-12-16T07:52:37.624884Z",
     "iopub.status.idle": "2020-12-16T07:52:37.662997Z",
     "shell.execute_reply": "2020-12-16T07:52:37.663518Z"
    },
    "papermill": {
     "duration": 0.062335,
     "end_time": "2020-12-16T07:52:37.663728",
     "exception": false,
     "start_time": "2020-12-16T07:52:37.601393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4307  728]\n",
      " [ 771 4194]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.86      0.85      5035\n",
      "        True       0.85      0.84      0.85      4965\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "0.8501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017341,
     "end_time": "2020-12-16T07:52:37.698997",
     "exception": false,
     "start_time": "2020-12-16T07:52:37.681656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train a different model : LogisticRegression\n",
    "\n",
    "Earlier we have trained a random forest classifier.\n",
    "Here, we'll train a **Logistic Regression Model** as our sentiment classifier.\n",
    "\n",
    "To do that, we'll first divide our dataset into **Train, Test and Validation sets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T07:52:37.743414Z",
     "iopub.status.busy": "2020-12-16T07:52:37.742652Z",
     "iopub.status.idle": "2020-12-16T07:52:38.670777Z",
     "shell.execute_reply": "2020-12-16T07:52:38.670108Z"
    },
    "papermill": {
     "duration": 0.953723,
     "end_time": "2020-12-16T07:52:38.670889",
     "exception": false,
     "start_time": "2020-12-16T07:52:37.717166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    processed_features, labels, train_size = 0.8, shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, train_size = 0.75, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017489,
     "end_time": "2020-12-16T07:52:38.706308",
     "exception": false,
     "start_time": "2020-12-16T07:52:38.688819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the Model over Train and Validation Sets\n",
    "In the previous window, we have divided dataset into three parts.\n",
    "Now, using train and validation sets, we'll train our logistic regression model.\n",
    "\n",
    "We'll try some values to get best values for C.\n",
    "We'll train and check which model gives highest accuracy and than using that value of C, we'll train our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T07:52:38.749006Z",
     "iopub.status.busy": "2020-12-16T07:52:38.748313Z",
     "iopub.status.idle": "2020-12-16T07:53:00.311182Z",
     "shell.execute_reply": "2020-12-16T07:53:00.310244Z"
    },
    "papermill": {
     "duration": 21.587048,
     "end_time": "2020-12-16T07:53:00.311335",
     "exception": false,
     "start_time": "2020-12-16T07:52:38.724287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8311\n",
      "Accuracy for C=0.05: 0.8606\n",
      "Accuracy for C=0.25: 0.879\n",
      "Accuracy for C=0.5: 0.8813\n",
      "Accuracy for C=1: 0.8851\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "# Accuracy for C=0.01: 0.8255\n",
    "# Accuracy for C=0.05: 0.8527\n",
    "# Accuracy for C=0.25: 0.8708\n",
    "# Accuracy for C=0.5: 0.877\n",
    "# Accuracy for C=1: 0.8777 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019461,
     "end_time": "2020-12-16T07:53:00.351053",
     "exception": false,
     "start_time": "2020-12-16T07:53:00.331592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Final Model and Calculate Accuracy over Test data\n",
    "\n",
    "* Now, we'll train the model over best value of C i.e. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-16T07:53:00.400713Z",
     "iopub.status.busy": "2020-12-16T07:53:00.399324Z",
     "iopub.status.idle": "2020-12-16T07:53:07.027097Z",
     "shell.execute_reply": "2020-12-16T07:53:07.026221Z"
    },
    "papermill": {
     "duration": 6.655928,
     "end_time": "2020-12-16T07:53:07.027309",
     "exception": false,
     "start_time": "2020-12-16T07:53:00.371381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=1: 0.8893\n"
     ]
    }
   ],
   "source": [
    "c = 1\n",
    "lr = LogisticRegression(C=c)\n",
    "lr.fit(X_train, y_train)\n",
    "print (\"Accuracy for C=%s: %s\" \n",
    "       % (c, accuracy_score(y_test, lr.predict(X_test))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.0204,
     "end_time": "2020-12-16T07:53:07.069475",
     "exception": false,
     "start_time": "2020-12-16T07:53:07.049075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 275.031556,
   "end_time": "2020-12-16T07:53:07.202585",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-16T07:48:32.171029",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
